{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages required\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a GET request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the url of the site\n",
    "base_site = \"http://www.assabahnews.tn/\"\n",
    "\n",
    "# Making a get request\n",
    "response = requests.get(base_site)\n",
    "#response.status_code \n",
    "#if 200 the everything is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the HTML\n",
    "html = response.content\n",
    "\n",
    "# Checking that the reply is indeed an HTML code by inspecting the first 100 symbols\n",
    "#html[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HTML to a BeautifulSoup object. This will allow us to parse out content from the HTML more easily.\n",
    "# Using the default parser as it is included in Python\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the HTML to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is extremely useful to be able to check this file when searching where some info is located\n",
    "# or to see how was the document parsed\n",
    "# Exporting the HTML to a file\n",
    "\n",
    "##with open('C:/Users/FAHD/Desktop/Formation/Web Scraping/AI Squad/BabNet/babnet.html', 'wb') as file:\n",
    "##    file.write(soup.prettify('utf-8'))\n",
    "\n",
    "# the 'with' statement is shorthand for a 'try-finally' block\n",
    "# open is function for opening/creating a file to edit\n",
    "# the 'wb' argument signifies the mode in which to edit the file - Writing in Bytes format\n",
    "# .prettify() modifies the HTML code with additional indentations for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching and navigating the HTML tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing videos section from our page\n",
    "videos = soup.find(\"section\", {'id':'block-views-op-recent-content-block-18'})\n",
    "videos.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can search for tags based on their attributes, in addition to their name\n",
    "x1 = soup.find_all('div', class_ = 'views-content-title')\n",
    "#len(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = soup.find_all('span', class_ = 'field-content')\n",
    "#len(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create e beautifulsoup object from our list \n",
    "aa = BeautifulSoup(str(x3), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the type of our variable\n",
    "#type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all links\n",
    "links = aa.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtained relative URLs\n",
    "# To obtain the absolute URL address we will use urljoin\n",
    "from urllib.parse import urljoin\n",
    "relative_urls = [link.get('href') for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to absolute path URLs\n",
    "full_urls = [urljoin(base_site, url) for url in relative_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple pages automatically - Extracting all the text from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective is to get all the useful text from those pages\n",
    "# We will do that by extracting all text contained in a paragraph element,\n",
    "# for all paragraphs on a page,\n",
    "# for all pages (in full_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to avoid limiting the number of requests, we should tellpython to wait between each request\n",
    "#we importtime library\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to store titles and articles for each webpage\n",
    "article = []\n",
    "titles = []\n",
    "types = []\n",
    "\n",
    "\n",
    "# creating a loop counter\n",
    "i = 0\n",
    "\n",
    "print('-------------------------- Beginning of Scraping --------------------------')\n",
    "\n",
    "# Loop through each URL in note_urls\n",
    "for url in full_urlss:\n",
    "    \n",
    "    #wait 3 secondes between requests\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # connect to every webpage\n",
    "    note_resp = requests.get(url)\n",
    "    \n",
    "    \n",
    "    # checking if the request is successful\n",
    "    if note_resp.status_code == 200:            # Everything is OK!\n",
    "        print('URL #{0}: {1}'.format(i+1,url))    # print out the number of iteration and the URL to keep track of place in loop\n",
    "    \n",
    "    else:                                       # Something is wrong!\n",
    "        print('Status code {0}: Skipping URL #{1}: {2}'.format(note_resp.status_code, i+1, url))\n",
    "        i = i+1\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # get HTML from webpage\n",
    "    note_html = note_resp.content\n",
    "    \n",
    "    # convert HTML to BeautifulSoup object\n",
    "    note_soup = BeautifulSoup(note_html, 'lxml')\n",
    "\n",
    "    \n",
    "    # find  articles on the webpage\n",
    "    note_pars = note_soup.find_all('div' , class_ = 'field-item even')\n",
    "    \n",
    "    # find the title\n",
    "    note_titles = note_soup.find('h1' , id = 'page-title')\n",
    "    \n",
    "    \n",
    "    # Transforming to text and cleaning every desired element in the page\n",
    "    art = [(p.text).replace('\\n','').replace('\\t','').replace(\"\\'\",'') for p in note_pars]\n",
    "    tit = [(t.string).replace('\\n','').replace('\\t','').replace('\\r','') for t in note_titles]\n",
    "    \n",
    "    # Append to our lists\n",
    "    article.append(str(art)[2:-2])\n",
    "    titles.append(str(tit)[2:-2])\n",
    "    types.append('-')\n",
    "    \n",
    "    # Incrementing the loop counter\n",
    "    i = i+1\n",
    "    \n",
    "print('-------------------------- SCRAPING DONE --------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to create our dataframe \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with our scraped data\n",
    "df = pd.DataFrame(list(zip(full_urls, titles, article, types)), columns =['link', 'titles', 'article', 'type']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and download the csv file\n",
    "path = ('C:/Users/FAHD/Desktop/Formation/Web Scraping/AI Squad/AssabahNews/AssabahNews.csv')\n",
    "df.to_csv(path, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
